---
layout: post
title: 算法基础 | 几种最优化方法详解
date: 2021-06-27 
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21627.html">原文地址：https://voidoc.blog/21627.html </a>



# 介绍  

大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和其它衍生算法等。



# 不同的最优化方法

##  1. 梯度下降法 （Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**

### 1.1 梯度

**【导数】**：导数我们都非常熟悉，既可以表示某点的**切线斜率**，也可以表示某点**变化率**，公式如下表示：

$f'\left(x\right)=\textstyle \lim_{\triangle x \to 0} \frac{\triangle y}{\triangle x}=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x\right) - f\left(x \right) }{\triangle x}$  

当函数是多元的，导数就变成了偏导数：如$f_{x} \left(x,y\right) $表示当y不变时，$f \left(x,y\right) $沿着x轴的变化率；那么同理，$f_{y} \left(x,y\right) $表示当x不变时，$f \left(x,y\right) $沿着y轴的变化率；其公式表达如下：  

$f_{x} \left(x,y\right)=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x, y\right) - f\left(x,y \right) }{\triangle x}$  

$f_{y} \left(x,y\right)=\textstyle \lim_{\triangle y \to 0} \frac{f\left(x,y + \triangle y\right) - f\left(x,y \right) }{\triangle y}$    

但是多元函数是一个平面，方向有很多， x轴、y轴只是其中两个方向而已，假如我们需要其他方向的变化率怎么办呢？这时候**方向导数**就有用了，顾名思义，方向导数可以表示任意方向的导数。  

**【方向导数】**：  

设：二次函数$f \left(x,y\right)$  ，方向（单位向量）$u=\cos \theta i + \sin \theta j$  ，那么该二次函数在这个方向下的导数为：$D_{u}f=\textstyle \lim_{t \to 0} \frac{f\left(x + t\cos\theta,y + t\sin\theta\right) - f\left(x,y \right) }{t}=f_{x}\left(x,y \right)\cos\theta + f_{y}\left(x,y \right)\sin\theta = [f_{x}\left(x,y \right), f_{y}\left(x,y \right)]\begin{bmatrix}\cos\theta \\\sin\theta\end{bmatrix}$    

我们记为$ D_{u}f = A\times I = abs(A)abs(I)\cos{\alpha} $,  其中$\alpha$ 是两向量的夹角。我们可以推断出当$\alpha$等于0时，方向导数$D_{u}f$达到最大值，此时的方向导数即为梯度。 

**【梯度的定义$\bigtriangledown$】**  
梯度的定义：某一函数沿着某点处的方向导数可以以最快速度到达极大值，该方向导数我们定义为该函数的梯度。  
$\bigtriangledown  = \frac{\mathrm{d} f\left ( \theta  \right ) }{\mathrm{d} \theta }  $  

其中$\theta$ 是自变量，根据泰勒展开式我们可以得出**梯度下降公式**：  
    $$\theta = \theta_{0}-\eta\cdot \bigtriangledown f(\theta_{0})$$

其中$\eta$是步长 。<a target="_blank" href="https://blog.csdn.net/weixin_42278173/article/details/81511646">[1]</a>

从几何意义上来说，梯度向量就是函数变化增加最快的地方。  

具体来说，对于函数$f \left(x,y\right)$，在给定点$\left(x_{0},y_{0}\right)$处，沿着梯度向量$\begin{bmatrix}\frac{\partial f}{\partial x_{0}}  \\ \frac{\partial f}{\partial y_{0}}  \end{bmatrix}$ 的方向就是该函数增加最快的地方。  

换个理解方式：沿着梯度向量的方向，更加容易找到函数的最大值。那么反过来说，沿着梯度向量相反的方向（去负号），则就是更加容易找到函数的最小值。  

### 1.2 梯度下降的直观解释

比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处（局部最小值）。  

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。  

### 1.3 梯度下降法迭代公式
了解了梯度下降的基本原理，我们来继续讲一下梯度下降法的具体实现。  
设$f\left(x\right)$是$R^{n}$ 上具有一阶连续偏导数的函数，我们的目标就是求解一个无约束最优化问题：$f^{\star} = \min_{x \in  R^{n}}f\left ( x \right )$ ， 其中$f^{\star}$就是目标函数要求的极小值。 

而梯度下降法是一种迭代算法。首先，我们需要选取适当的初值 $x^{\left (0  \right ) }$ ，然后不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。根据1.2可知，**负梯度方向是使函数下降最快的的方向**，所以在迭代的每一步，我们需要以负梯度方向更新$x$的值，从而达到减少函数值的目的。  

举个栗子，在梯度下降法的第k+1次迭代值为：
$$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda _{k}p_{k}$$  
其中$p_{k}=-\bigtriangledown f\left (  x^\left ( k \right ) \right ) $，是搜索方向，即负梯度方向；  
其中$\lambda_{k}$是**搜索步长(即stride即学习率lr)**，需要满足$ f\left(x^{\left(k\right)}+\lambda_{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda_{k} p_{k} \right ) $  


> 梯度下降法步骤 

**【输入】**： 目标函数$f\left(x\right)$,  其梯度函数$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) $, 计算精度$\epsilon$  
**【输出】**：  $f\left(x\right)$的极小点$x^{\star}$  
**【步骤】**：  
1. 取初始值$x^{\left ( 0 \right ) }\in R^{n}$, 置k=0
2. 计算$f\left ( x^{\left (k\right )} \right )$
3. 计算梯度$g_{k}=g\left ( x^{\left (k\right )} \right )$  
当$\left \| g_{k}\right \| \le \varepsilon$ 时，停止迭代，且令$x^{\star}=x^\left ( k \right )$    
否则，令搜索方向$p_{k}=-g\left ( x^{\left ( k \right ) } \right ) $等于负梯度方向，然后求$\lambda_{k}$,使 ：
$$ f\left (x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) $$  
4. 下一步，置$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda_{k}p_{k}$，然后继续计算$f\left ( x^{\left ( k+1 \right ) } \right ) $   
当$\left \| f\left ( x^{\left ( k+1 \right )} \right ) - f\left ( x^{\left ( k \right )} \right )  \right \| \le  \varepsilon $ 或 $\left \|  x^{\left ( k+1 \right )}  -  x^{\left ( k \right )}   \right \| \le  \varepsilon $时，停止迭代，且令$x^{\star}=x^\left ( k+1 \right )$    
否则k=k+1，转步骤3。  

> 具体例子   

**单变量示例**  

现有函数$J\left ( x \right ) = x^{2}$  
那么导数（单变量情况下也就是他的梯度）为$J'\left ( x \right ) = 2x$  
所以梯度函数就是$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) =2x^{\left (k  \right ) }$  

设初始点$x^{0}= 1$，设置搜索步长（即学习率）$\alpha=0.4$  
根据梯度下降\公式，迭代过程如下：  
$x^{0}=1$   
$x^{1}=x^{0}-\alpha J'\left ( x^{0} \right )=1-0.4\ast 2 =0.2$    
$x^{2}=x^{1}-\alpha J'\left ( x^{1} \right )=0.2-0.4\ast 0.4 =0.04$     
$x^{3}=x^{2}-\alpha J'\left ( x^{2} \right )=0.04-0.4\ast 0.08 =0.008$    
$x^{4}=x^{3}-\alpha J'\left ( x^{3} \right )=0.008-0.4\ast 0.016 =0.0016$  
$J'\left ( x^{4} \right )=2\ast0.0016=0.0032\approx 0$ 基本找到最低点  


**多变量示例**   
现有函数$J\left ( x \right ) = x^{2}+y^{2}$   
那么他的梯度函数为$\bigtriangledown J\left ( x,y \right ) = \bigtriangledown J\left ( \theta  \right )=<2x,2y>$  
设初始点$\theta^{0}= \left ( 1,3 \right ) $，设置搜索步长（即学习率）$\alpha=0.1$  
根据梯度下降公式，迭代过程如下：  
$\theta^{0}= \left ( 1,3 \right ) $  
$\theta^{1}= \theta^{0} - \alpha\bigtriangledown J\left (\theta^{0}  \right ) = \begin{bmatrix}
0.8 \\ 2.4 \end{bmatrix} $  
$\theta^{2}= \theta^{1} - \alpha\bigtriangledown J\left (\theta^{1}  \right ) = \begin{bmatrix}
0.64 \\ 1.92 \end{bmatrix} $    
$\theta^{3}= \theta^{2} - \alpha\bigtriangledown J\left (\theta^{2}  \right ) = \begin{bmatrix}
0.512 \\ 1.536 \end{bmatrix} $    
……  
$\bigtriangledown J\left (\theta^{100}  \right ) \approx 0$     此时梯度接近0，因此在k=100时达到了局部最小值。  



### 1.4 梯度下降算法的调优
在使用梯度下降时，需要进行调优。哪些地方需要调优呢？  

**1. 算法的步长选择** ：步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。   

**2. 算法参数的初始值选择** ：初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小（当然如果损失函数是凸函数则一定是最优解）。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 

**3. 归一化** ：由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。

### 1.5 梯度下降法的缺点

　　**1. 靠近极小值时收敛速度减慢，如下图所示；**

　　**2. 直线搜索时可能会产生一些问题；**

　　**3. 寻找的是局部最优，可能会“之字形”地下降。**
　
### 1.6 梯度下降法大家族（BGD，SGD，MBGD）
- **批量梯度下降法（Batch Gradient Descent）**   

批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。 这样一来每迭代一步，都要用到训练集所有的数据，如果数据量很大，那么可想而知这种方法的迭代速度会很慢。所以，这就引入了另外一种方法，随机梯度下降。

- **随机梯度下降法（Stochastic Gradient Descent）**   

随机梯度下降（Stochastic Gradient Descent）每次迭代只用到了一个样本，在样本量很大的情况下，常见的情况是只用到了其中一部分样本数据即可迭代到最优解。因此随机梯度下降比批量梯度下降在计算量上会大大减少。SGD有一个缺点是，其噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。而且SGD因为每次都是使用一个样本进行迭代，因此最终求得的最优解往往不是全局最优解，而只是局部最优解。但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。  

- **小批量梯度下降法（Mini-batch Gradient Descent）**    

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用 x个样子来迭代，1<x<m 。   

> 以多元线性回归为例子    

设有线性函数 $f\left ( x \right ) = \omega_{0}+\omega_{1}x_{1}+\dots +\omega_{n}x_{n} = \omega^{T}x$  
那么损失函数就是真实值跟预测值的MSE: $J\left ( \omega \right ) = \sum_{i=1}^{n}\left ( \omega^{T}x^{i} -y^{i}\right )^{2} $  
那么梯度就是 $\bigtriangledown J\left ( \omega \right )=\frac{\partial J\left ( \omega \right )}{\partial \omega}  = \sum_{i=1}^{n}2\left ( \omega^{T}x^{i} -y^{i}\right )\ast x_{j}^{i}$  

那么**批量梯度下降法（Batch Gradient Descent）**的更新规则就是：  
    $$\omega_{j} = \omega_{j} -  2\alpha\sum_{i=1}^{n}\left ( \omega^{T}x^{i} -y^{i}\right )\ast x_{j}^{i}$$  


**小批量梯度下降法（Mini-batch Gradient Descent）**的更新规则就是：  
    $$\omega_{j} = \omega_{j} -  2\alpha\sum_{i=1}^{m}\left ( \omega^{T}x^{i} -y^{i}\right )\ast x_{j}^{i}$$  

而**随机梯度下降法（Stochastic Gradient Descent）** 的更新规则则为：  
    $\omega_{j} = \omega_{j} -  2\alpha\left ( \omega^{T}x^{i} -y^{i}\right )\ast x_{j}^{i}$  
根据loss图像我们可以看到，BGD的loss是一直下降的，而SGD的loss虽然整体是下降的，但是中途有升有降<a target="_blank" href="https://blog.csdn.net/weixin_42278173/article/details/81511646">[2]</a>    

### 1.7 批量梯度下降法python实现

```python
def train(X, y, W, B, alpha, max_iters):
    '''
    使用了所有的样本进行梯度下降
    X: 训练集,
    y: 标签,
    W: 权重向量,
    B: bias,
    alpha: 学习率,
    max_iters: 最大迭代次数.
    '''
    dW = 0 # 权重梯度收集器
    dB = 0 # Bias梯度的收集器
    m = X.shape[0] # 样本数
    
    for i in range(max_iters):
        dW = 0 # 每次迭代重置
        dB = 0
        for j in range(m):
            # 1. 迭代所有的样本
            # 2. 计算权重和bias的梯度保存在w_grad和b_grad,
            # 3. 通过增加w_grad和b_grad来更新dW和dB
            W = W - alpha * (dW / m) # 更新权重
            B = B - alpha * (dB / m) # 更新bias
    return W, B 
```



##  2. 牛顿法（Newton's method）  

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f(x)的泰勒级数的前面几项来寻找方程f(x)=0的根。牛顿法最大的特点就在于它的收敛速度很快。

### 2.1 牛顿法的求解方程  

并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以**迭代求解**。原理是利用泰勒公式<a target="_blank" href="https://baike.baidu.com/item/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F">[3]</a>, 在$x_{0}$处展开到一阶，即$f\left ( x \right )=f\left (  x_{0} \right ) + f'\left (  x_{0} \right )\left ( x-x_{0} \right ) $ **【式2.1】**  

然后求解$f\left ( x \right )=0$，解得$x = x_{1}=x_{0} - \frac{f\left ( x_{0} \right )}{f'\left ( x_{0} \right )} $。  
这是利用泰勒公式一阶展开求得的近似解，所以$x_{1}$并不能让${f\left ( x \right )} =0$  
只能说${f\left ( x_{1} \right )}$的值比${f\left ( x_{0} \right )}$的值更接近于0， 同理，${f\left ( x_{2} \right )}$的值比${f\left ( x_{1} \right )}$的值更接近于0，etc...  
所以迭代的想法因此而生，进而推出了公式：$x_{n+1}=x_{n} - \frac{f\left ( x_{n} \right )}{f'\left ( x_{n} \right )} $  
通过迭代，该式在${f\left ( x^{\ast } \right )}=0 $的时候收敛，此时我们得方程根。  
### 2.2 牛顿法的迭代公式 

同样地，上述求解方程的过程也可以看作是一个**无约束最优化问题**：$\min_{x\subset R^{n}}f\left ( x \right ) $  ，我们要找的也就是目标函数的极小点$x^{\ast} $。  

设，${f\left ( x\right )}$具有二阶连续偏导数，那么我们就可以对**【式2.1】**两边取导数，得：  
$f'\left ( x \right ) =f'\left ( x_{0} \right ) + f''\left ( x_{0} \right )\left ( x-x_{0} \right )$,设$\bigtriangleup x = x - x_{0}$  
\Leftrightarrow   

$f'\left ( x \right ) =f'\left ( x_{0} \right ) + f''\left ( x_{0} \right )\bigtriangleup x$  
根据微积分的性质，f(x)取最小值时，有f'(x)=0，我们把这个性质代入上面的式子，有:  
$0 =f'\left ( x_{0} \right ) + f''\left ( x_{0} \right )\bigtriangleup x$  
\Leftrightarrow   
$\bigtriangleup x=-\frac{f'\left ( x_{0} \right )}{ f''\left ( x_{0} \right )}$  

这样我们就得到了牛顿法的参数迭代更新公式如下:  
$x_{n+1}=x_{n}-\frac{f'\left ( x_{n} \right )}{ f''\left ( x_{n} \right )}=x_{n} - \frac{\bigtriangledown f\left ( x_{n} \right ) }{H\left ( f\left ( x_{n} \right ) \right ) } $  **【式2.2】**   
其中H就是f(x)的Hessian海塞矩阵<a target="_blank" href="https://baike.baidu.com/item/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5">[3]</a>    

> 牛顿法步骤   
 
**【输入】**： 目标函数$f\left(x\right)$,  其梯度函数$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) $, 计算精度$\epsilon$  
**【输出】**：  $f\left(x\right)$的极小点$x^{\star}$  
**【步骤】**：  
1. 取初始值$x^{\left ( 0 \right ) }\in R^{n}$, 置k=0  
2. 计算梯度$g_{k}=g\left ( x^{\left (k\right )} \right )$    
3. 若$\left \| g_{k}\right \| \le \varepsilon$ ，停止迭代，求得近似解$x^{\star}=x^\left ( k \right )$     
4. 计算  $H_{k}=H\left ( x^{\left ( k \right ) } \right ) $  
5. 置$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } - \frac{g_{k}}{H_{k}} $  
6. 置k=k+1，转步骤2。   


> 具体例子  

用牛顿法求解$\min f\left ( x \right ) = \frac{1}{2} x_{1}^{2}+\frac{9}{2} x_{2}^{2},x^{0}= \begin{bmatrix} 9 \\ 1 \end{bmatrix}$  
* 计算梯度$\bigtriangledown f\left ( x \right ) = \begin{bmatrix} \frac{\partial f}{\partial x_{1}} \\ \frac{\partial f}{\partial x_{2}} \end{bmatrix}=\begin{bmatrix}x_{1} \\ 9x_{2}\end{bmatrix}$   
* 计算海塞矩阵$Hf\left ( x \right ) =\begin{bmatrix}\frac{\partial^2f}{\partial x_1^{2}}&\frac{\partial^2f}{\partial x_1\partial x_2} \\\frac{\partial^2f}{\partial x_2\partial x_1} &\frac{\partial^2f}{\partial x_2^{2}} \end{bmatrix}=\begin{bmatrix}1 & 0 \\0 & 9\end{bmatrix}$  
* 所以$x^{1}=x^{0} -\frac{\bigtriangledown f\left ( x^{0} \right ) }{H\left ( f\left ( x^{0} \right ) \right ) }=\begin{bmatrix}9 \\ 1 \end{bmatrix}$ - \begin{bmatrix} 1 & 0 \\ 0 & 9 \end{bmatrix}^{-1}\begin{bmatrix}9 \\ 9 \end{bmatrix}=\begin{bmatrix}0 \\ 0 \end{bmatrix}$  
* $\left \| \bigtriangledown f\left ( x^1 \right ) \right \| =0 \le \varepsilon$,所以迭代终止，极小点为$x^1=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$  
 
 
 
### 2.3 牛顿法与梯度下降法的差异  
总的来说，梯度法和牛顿法有如下区别：  

* 梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的海森矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长。（至于为什么牛顿法收敛更快，通俗来说梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。）  


* 牛顿法对初始值有一定要求，在非凸优化问题中（如神经网络训练），牛顿法很容易陷入鞍点（牛顿法步长会越来越小），而梯度下降法则很容易逃离鞍点（因此在神经网络训练中一般使用梯度下降法，高维空间的神经网络中存在大量鞍点）  
* 梯度下降法在靠近最优点时会震荡，因此步长调整在梯度下降法中是必要的，具体有adagrad, adadelta, rmsprop, adam等一系列自适应学习率的方法。  

### 2.4 拟牛顿法  

在牛顿法的迭代中，需要计算海森矩阵的逆矩阵$H^{-1}$ ，这一计算比较复杂，考虑用一个n阶矩阵$G_{k}=G\left ( x^{\left ( k \right ) } \right ) $来近似代替$H_{k}^{-1}=H^{-1}\left ( x^{\left ( k \right ) } \right ) $。这就是拟牛顿法的基本想法。  
要找到近似的替代矩阵，必定要和$H_{k}$有类似的性质。先看下牛顿法迭代中海森矩阵$H_{k}$满足的条件。首先$H_{k}$满足以下关系： 



-----


引用：
1. 梯度下降法、牛顿法和拟牛顿法：<a target="_blank" href="https://zhuanlan.zhihu.com/p/37524275">https://zhuanlan.zhihu.com/p/37524275 </a>
2. 梯度下降法<a target="_blank" href="https://blog.csdn.net/weixin_42278173/article/details/81511646">https://blog.csdn.net/weixin_42278173/article/details/81511646</a>
3. 泰勒公式<a target="_blank" href="https://baike.baidu.com/item/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F">https://baike.baidu.com/item/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F</a>
4.海塞矩阵<a target="_blank" href="https://baike.baidu.com/item/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5">https://baike.baidu.com/item/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5</a>


