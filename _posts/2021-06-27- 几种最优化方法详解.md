---
layout: post
title: 算法基础 | 几种最优化方法详解
date: 2021-06-27 
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21627.html">原文地址：https://voidoc.blog/21627.html </a>



# 介绍  

大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和其它衍生算法等。

-----



# 不同的最优化方法

##  1. 梯度下降法 （Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**

### 1.1 梯度

【导数】：导数我们都非常熟悉，既可以表示某点的**切线斜率**，也可以表示某点**变化率**，公式如下表示：

$f'\left(x\right)=\textstyle \lim_{\triangle x \to 0} \frac{\triangle y}{\triangle x}=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x\right) - f\left(x \right) }{\triangle x}$  

当函数是多元的，导数就变成了偏导数：如$f_{x} \left(x,y\right) $表示当y不变时，$f \left(x,y\right) $沿着x轴的变化率；那么同理，$f_{y} \left(x,y\right) $表示当x不变时，$f \left(x,y\right) $沿着y轴的变化率；其公式表达如下：  

$f_{x} \left(x,y\right)=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x, y\right) - f\left(x,y \right) }{\triangle x}$  

$f_{y} \left(x,y\right)=\textstyle \lim_{\triangle y \to 0} \frac{f\left(x,y + \triangle y\right) - f\left(x,y \right) }{\triangle y}$    

但是多元函数是一个平面，方向有很多， x轴、y轴只是其中两个方向而已，假如我们需要其他方向的变化率怎么办呢？这时候**方向导数**就有用了，顾名思义，方向导数可以表示任意方向的导数。  

设：二次函数$f \left(x,y\right)$  ，方向（单位向量）$u=\cos \theta i + \sin \theta j$  ，那么该二次函数在这个方向下的导数为：$D_{u}f=\textstyle \lim_{t \to 0} \frac{f\left(x + t\cos\theta,y + t\sin\theta\right) - f\left(x,y \right) }{t}=f_{x}\left(x,y \right)\cos\theta + f_{y}\left(x,y \right)\sin\theta = [f_{x}\left(x,y \right), f_{y}\left(x,y \right)]\begin{bmatrix}\cos\theta \\\sin\theta\end{bmatrix}$    

【梯度】：我们记为：$D_{u}f=A\times I = \left | A \right |\left | I \right | \cos\alpha $   ，其中\alpha 是两向量的夹角。我们可以推断出，当\alpha等于0时，方向导数$D_{u}f$达到最大值，此时的方向导数即为梯度。  

从几何意义上来说，梯度向量就是函数变化增加最快的地方。  

具体来说，对于函数$f \left(x,y\right)$，在给定点$\left(x_{0},y_{0}\right)$处，沿着梯度向量$\begin{bmatrix}\frac{\partial f}{\partial x_{0}}  \\\frac{\partial f}{\partial y_{0}}  \end{bmatrix}$     的方向就是该函数增加最快的地方。  

换个理解方式：沿着梯度向量的方向，更加容易找到函数的最大值。那么反过来说，沿着梯度向量相反的方向（去负号），则就是更加容易找到函数的最小值。  



**梯度下降法的缺点：**

　　**（1）靠近极小值时收敛速度减慢，如下图所示；**

　　**（2）直线搜索时可能会产生一些问题；**

　　**（3）可能会“之字形”地下降。**


-----

引用：

1. 梯度下降法、牛顿法和拟牛顿法：<a target="_blank" href="https://zhuanlan.zhihu.com/p/37524275">https://zhuanlan.zhihu.com/p/37524275 </a>
