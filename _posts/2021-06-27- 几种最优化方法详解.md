---
layout: post
title: 算法基础 | 几种最优化方法详解
date: 2021-06-27 
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21627.html">原文地址：https://voidoc.blog/21627.html </a>



# 介绍  

大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和其它衍生算法等。



# 不同的最优化方法

##  1. 梯度下降法 （Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**

### 1.1 梯度

**【导数】**：导数我们都非常熟悉，既可以表示某点的**切线斜率**，也可以表示某点**变化率**，公式如下表示：

$f'\left(x\right)=\textstyle \lim_{\triangle x \to 0} \frac{\triangle y}{\triangle x}=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x\right) - f\left(x \right) }{\triangle x}$  

当函数是多元的，导数就变成了偏导数：如$f_{x} \left(x,y\right) $表示当y不变时，$f \left(x,y\right) $沿着x轴的变化率；那么同理，$f_{y} \left(x,y\right) $表示当x不变时，$f \left(x,y\right) $沿着y轴的变化率；其公式表达如下：  

$f_{x} \left(x,y\right)=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x, y\right) - f\left(x,y \right) }{\triangle x}$  

$f_{y} \left(x,y\right)=\textstyle \lim_{\triangle y \to 0} \frac{f\left(x,y + \triangle y\right) - f\left(x,y \right) }{\triangle y}$    

但是多元函数是一个平面，方向有很多， x轴、y轴只是其中两个方向而已，假如我们需要其他方向的变化率怎么办呢？这时候**方向导数**就有用了，顾名思义，方向导数可以表示任意方向的导数。  

**【方向导数】**：  

设：二次函数$f \left(x,y\right)$  ，方向（单位向量）$u=\cos \theta i + \sin \theta j$  ，那么该二次函数在这个方向下的导数为：$D_{u}f=\textstyle \lim_{t \to 0} \frac{f\left(x + t\cos\theta,y + t\sin\theta\right) - f\left(x,y \right) }{t}=f_{x}\left(x,y \right)\cos\theta + f_{y}\left(x,y \right)\sin\theta = [f_{x}\left(x,y \right), f_{y}\left(x,y \right)]\begin{bmatrix}\cos\theta \\\sin\theta\end{bmatrix}$    

我们记为$ D_{u}f = A\times I = abs(A)abs(I)\cos{\alpha} $,  其中$\alpha$ 是两向量的夹角。我们可以推断出当$\alpha$等于0时，方向导数$D_{u}f$达到最大值，此时的方向导数即为梯度。 

**【梯度的定义$\bigtriangledown$】**  
梯度的定义：某一函数沿着某点处的方向导数可以以最快速度到达极大值，该方向导数我们定义为该函数的梯度。  
$\bigtriangledown  = \frac{\mathrm{d} f\left ( \theta  \right ) }{\mathrm{d} \theta }  $  

其中$\theta$ 是自变量，根据泰勒展开式我们可以得出**梯度下降公式**：  
    **$$\theta = \theta_{0}-\eta\cdot \bigtriangledown f(\theta_{0})$$*

其中$\eta$是步长 。<a target="_blank" href="https://blog.csdn.net/weixin_42278173/article/details/81511646">[1]</a>

从几何意义上来说，梯度向量就是函数变化增加最快的地方。  

具体来说，对于函数$f \left(x,y\right)$，在给定点$\left(x_{0},y_{0}\right)$处，沿着梯度向量$\begin{bmatrix}\frac{\partial f}{\partial x_{0}}  \\ \frac{\partial f}{\partial y_{0}}  \end{bmatrix}$ 的方向就是该函数增加最快的地方。  

换个理解方式：沿着梯度向量的方向，更加容易找到函数的最大值。那么反过来说，沿着梯度向量相反的方向（去负号），则就是更加容易找到函数的最小值。  

### 1.2 梯度下降的直观解释

比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处（局部最小值）。  

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。  

### 1.3 梯度下降法
了解了梯度下降的基本原理，我们来继续讲一下梯度下降法的具体实现。  
设$f\left(x\right)$是$R^{n}$ 上具有一阶连续偏导数的函数，我们的目标就是求解一个无约束最优化问题：$f^{\star} = \min_{x \in  R^{n}}f\left ( x \right )$ ， 其中$f^{\star}$就是目标函数要求的极小值。 

而梯度下降法是一种迭代算法。首先，我们需要选取适当的初值 $x^{\left (0  \right ) }$ ，然后不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。根据1.2可知，**负梯度方向是使函数下降最快的的方向**，所以在迭代的每一步，我们需要以负梯度方向更新$x$的值，从而达到减少函数值的目的。  

举个栗子，在梯度下降法的第k+1次迭代值为：
$$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda _{k}p_{k}$$  
其中$p_{k}=-\bigtriangledown f\left (  x^\left ( k \right ) \right ) $，是搜索方向，即负梯度方向；  
其中$\lambda_{k}$是**搜索步长(即stride即学习率lr)**，需要满足$ f\left(x^{\left(k\right)}+\lambda_{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda_{k} p_{k} \right ) $  


> 梯度下降法步骤 

**【输入】**： 目标函数$f\left(x\right)$,  其梯度函数$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) $, 计算精度$\epsilon$  
**【输出】**：  $f\left(x\right)$的极小点$x^{\star}$  
**【步骤】**：  
1. 取初始值$x^{\left ( 0 \right ) }\in R^{n}$, 置k=0
2. 计算$f\left ( x^{\left (k\right )} \right )$
3. 计算梯度$g_{k}=g\left ( x^{\left (k\right )} \right )$  
当$\left \| g_{k}\right \| \le \varepsilon$ 时，停止迭代，且令$x^{\star}=x^\left ( k \right )$    
否则，令搜索方向$p_{k}=-g\left ( x^{\left ( k \right ) } \right ) $等于负梯度方向，然后求$\lambda_{k}$,使 ：
$$ f\left (x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) $$  
4. 下一步，置$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda_{k}p_{k}$，然后继续计算$f\left ( x^{\left ( k+1 \right ) } \right ) $   
当$\left \| f\left ( x^{\left ( k+1 \right )} \right ) - f\left ( x^{\left ( k \right )} \right )  \right \| \le  \varepsilon $ 或 $\left \|  x^{\left ( k+1 \right )}  -  x^{\left ( k \right )}   \right \| \le  \varepsilon $时，停止迭代，且令$x^{\star}=x^\left ( k+1 \right )$    
否则k=k+1，转步骤3。  

> 具体例子   

**单变量示例**  

现有函数$J\left ( x \right ) = x^{2}$  
那么导数（单变量情况下也就是他的梯度）为$J'\left ( x \right ) = 2x$  
所以梯度函数就是$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) =2x^{\left (k  \right ) }$  

设初始点$x^{0}= 1$，设置搜索步长（即学习率）$\alpha=0.4$  
根据梯度下降\公式，迭代过程如下：  
$x^{0}=1$   
$x^{1}=x^{0}-\alpha J'\left ( x^{0} \right )=1-0.4\ast 2 =0.2$    
$x^{2}=x^{1}-\alpha J'\left ( x^{1} \right )=0.2-0.4\ast 0.4 =0.04$     
$x^{3}=x^{2}-\alpha J'\left ( x^{2} \right )=0.04-0.4\ast 0.08 =0.008$    
$x^{4}=x^{3}-\alpha J'\left ( x^{3} \right )=0.008-0.4\ast 0.016 =0.0016$  
$J'\left ( x^{4} \right )=2\ast0.0016=0.0032\approx 0$ 基本找到最低点  
 

**多变量示例**   
现有函数$J\left ( x \right ) = x^{2}+y^{2}$   
那么他的梯度函数为$\bigtriangledown J\left ( x,y \right ) = \bigtriangledown J\left ( \theta  \right )=<2x,2y>$  
设初始点$\theta^{0}= \left ( 1,3 \right ) $，设置搜索步长（即学习率）$\alpha=0.1$  
根据梯度下降\公式，迭代过程如下：  
$\theta^{0}= \left ( 1,3 \right ) $  
$\theta^{1}= \theta^{0} - \alpha\bigtriangledown J\left (\theta^{0}  \right ) = \begin{bmatrix}
0.8 \\ 2.4 \end{bmatrix} $  
$\theta^{2}= \theta^{1} - \alpha\bigtriangledown J\left (\theta^{1}  \right ) = \begin{bmatrix}
0.64 \\ 1.92 \end{bmatrix} $    
$\theta^{3}= \theta^{2} - \alpha\bigtriangledown J\left (\theta^{2}  \right ) = \begin{bmatrix}
0.512 \\ 1.536 \end{bmatrix} $    
……  
$\bigtriangledown J\left (\theta^{100}  \right ) \approx 0$     此时梯度接近0，因此在k=100时达到了局部最小值。  



### 1.4 梯度下降算法的调优
在使用梯度下降时，需要进行调优。哪些地方需要调优呢？  

**1. 算法的步长选择** ：步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。   

**2. 算法参数的初始值选择** ：初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 

**3. 归一化** ：由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。

### 1.5 梯度下降法的缺点

　　**1. 靠近极小值时收敛速度减慢，如下图所示；**

　　**2. 直线搜索时可能会产生一些问题；**

　　**3. 寻找的是局部最优，可能会“之字形”地下降。**
　
### 1.6 梯度下降法大家族（BGD，SGD，MBGD）
- **批量梯度下降法（Batch Gradient Descent）**  
批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新。
- **随机梯度下降法（Stochastic Gradient Descent）**   

随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的样本的数据，而是仅仅选取一个样本来求梯度。  
随机梯度下降法和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。  
自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。  

- **小批量梯度下降法（Mini-batch Gradient Descent）**   
小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用 x个样子来迭代，1<x<m 。  

### 1.7 批量梯度下降法python实现

```python
def train(X, y, W, B, alpha, max_iters):
    '''
    使用了所有的样本进行梯度下降
    X: 训练集,
    y: 标签,
    W: 权重向量,
    B: bias,
    alpha: 学习率,
    max_iters: 最大迭代次数.
    '''
    dW = 0 # 权重梯度收集器
    dB = 0 # Bias梯度的收集器
    m = X.shape[0] # 样本数
    
    for i in range(max_iters):
        dW = 0 # 每次迭代重置
        dB = 0
        for j in range(m):
            # 1. 迭代所有的样本
            # 2. 计算权重和bias的梯度保存在w_grad和b_grad,
            # 3. 通过增加w_grad和b_grad来更新dW和dB
            W = W - alpha * (dW / m) # 更新权重
            B = B - alpha * (dB / m) # 更新bias
    return W, B 
```

-----

##  2. 牛顿法（Newton's method）  

牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。
-----
引用：
1. 梯度下降法、牛顿法和拟牛顿法：<a target="_blank" href="https://zhuanlan.zhihu.com/p/37524275">https://zhuanlan.zhihu.com/p/37524275 </a>
2. 梯度下降法<a target="_blank" href="https://blog.csdn.net/weixin_42278173/article/details/81511646">https://blog.csdn.net/weixin_42278173/article/details/81511646</a>

