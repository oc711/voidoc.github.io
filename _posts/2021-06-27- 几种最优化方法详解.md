---
layout: post
title: 算法基础 | 几种最优化方法详解
date: 2021-06-27 
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21627.html">原文地址：https://voidoc.blog/21627.html </a>



# 介绍  

大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和其它衍生算法等。



# 不同的最优化方法

##  1. 梯度下降法 （Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**

### 1.1 梯度

**【导数】**：导数我们都非常熟悉，既可以表示某点的**切线斜率**，也可以表示某点**变化率**，公式如下表示：

$f'\left(x\right)=\textstyle \lim_{\triangle x \to 0} \frac{\triangle y}{\triangle x}=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x\right) - f\left(x \right) }{\triangle x}$  

当函数是多元的，导数就变成了偏导数：如$f_{x} \left(x,y\right) $表示当y不变时，$f \left(x,y\right) $沿着x轴的变化率；那么同理，$f_{y} \left(x,y\right) $表示当x不变时，$f \left(x,y\right) $沿着y轴的变化率；其公式表达如下：  

$f_{x} \left(x,y\right)=\textstyle \lim_{\triangle x \to 0} \frac{f\left(x + \triangle x, y\right) - f\left(x,y \right) }{\triangle x}$  

$f_{y} \left(x,y\right)=\textstyle \lim_{\triangle y \to 0} \frac{f\left(x,y + \triangle y\right) - f\left(x,y \right) }{\triangle y}$    

但是多元函数是一个平面，方向有很多， x轴、y轴只是其中两个方向而已，假如我们需要其他方向的变化率怎么办呢？这时候**方向导数**就有用了，顾名思义，方向导数可以表示任意方向的导数。  

设：二次函数$f \left(x,y\right)$  ，方向（单位向量）$u=\cos \theta i + \sin \theta j$  ，那么该二次函数在这个方向下的导数为：$D_{u}f=\textstyle \lim_{t \to 0} \frac{f\left(x + t\cos\theta,y + t\sin\theta\right) - f\left(x,y \right) }{t}=f_{x}\left(x,y \right)\cos\theta + f_{y}\left(x,y \right)\sin\theta = [f_{x}\left(x,y \right), f_{y}\left(x,y \right)]\begin{bmatrix}\cos\theta \\\sin\theta\end{bmatrix}$    

**【梯度$\bigtriangledown$】**：我们记为$D_{u}f = \mathrm {A} \times \mathrm {I}  = \left | \mathrm {A} \right | \left | \mathrm {I}\right | \cos \alpha $

其中$\alpha$ 是两向量的夹角。我们可以推断出，当$\alpha$等于0时，方向导数$D_{u}f$达到最大值，此时的方向导数即为梯度。 

从几何意义上来说，梯度向量就是函数变化增加最快的地方。  

具体来说，对于函数$f \left(x,y\right)$，在给定点$\left(x_{0},y_{0}\right)$处，沿着梯度向量$\begin{bmatrix}\frac{\partial f}{\partial x_{0}}  \\ \frac{\partial f}{\partial y_{0}}  \end{bmatrix}$ 的方向就是该函数增加最快的地方。  

换个理解方式：沿着梯度向量的方向，更加容易找到函数的最大值。那么反过来说，沿着梯度向量相反的方向（去负号），则就是更加容易找到函数的最小值。  

### 1.2 梯度下降的直观解释

比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处（局部最小值）。  

从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。  

### 1.3 梯度下降法
了解了梯度下降的基本原理，我们来继续讲一下梯度下降法的具体实现。  
设$f\left(x\right)$是$R^{n}$ 上具有一阶连续偏导数的函数，我们的目标就是求解一个无约束最优化问题：$f^{\star} = \min_{x \in  R^{n}}f\left ( x \right )$ ， 其中$f^{\star}$就是目标函数要求的极小值。 

而梯度下降法是一种迭代算法。首先，我们需要选取适当的初值 $x^{\left (0  \right ) }$ ，然后不断迭代，更新$x$的值，进行目标函数的极小化，直到收敛。根据1.2可知，**负梯度方向是使函数下降最快的的方向**，所以在迭代的每一步，我们需要以负梯度方向更新$x$的值，从而达到减少函数值的目的。  

举个栗子，在梯度下降法的第k+1次迭代值为：
$$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda _{k}p_{k}$$  
其中$p_{k}=-\bigtriangledown f\left (  x^\left ( k \right ) \right ) $，是搜索方向，即负梯度方向；  
其中$\lambda_{k}$是搜索步长，需要满足$ f\left(x^{\left(k\right)}+\lambda_{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda_{k} p_{k} \right ) $  


> 梯度下降法步骤 

**【输入】**： 目标函数$f\left(x\right)$,  其梯度函数$g\left ( x^{\left (k\right )} \right )=\bigtriangledown  f\left ( x^{\left ( k \right ) } \right ) $,, 计算精度$\epsilon$  
**【输出】**：  $f\left(x\right)$的极小点$x^{\star}$  
**【步骤】**：  
1. 取初始值$x^{\left ( 0 \right ) }\in R^{n}$, 置k=0
2. 计算$f\left ( x^{\left (k\right )} \right )$
3. 计算梯度$g_{k}=g\left ( x^{\left (k\right )} \right )$  
当$\left \| g_{k}\right \| \le \varepsilon$ 时，停止迭代，且令$x^{\star}=x^\left ( k \right )$    
否则，令搜索方向$p_{k}=-g\left ( x^{\left ( k \right ) } \right ) $等于负梯度方向，然后求$\lambda_{k}$,使 ：
$$ f\left (x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) =\min_{\lambda \ge 0}f\left ( x^{\left ( k \right ) }+ \lambda _{k} p_{k} \right ) $$  
4. 下一步，置$x^{\left ( k+1 \right ) }=x^{\left ( k \right ) } + \lambda_{k}p_{k}$，然后继续计算$f\left ( x^{\left ( k+1 \right ) } \right ) $   
当$\left \| f\left ( x^{\left ( k+1 \right )} \right ) - f\left ( x^{\left ( k \right )} \right )  \right \| \le  \varepsilon $ 或 $\left \|  x^{\left ( k+1 \right )}  -  x^{\left ( k \right )}   \right \| \le  \varepsilon $时，停止迭代，且令$x^{\star}=x^\left ( k+1 \right )$    
否则k=k+1，转步骤3。  

-----  

### 1.4 梯度下降算法的调优
在使用梯度下降时，需要进行调优。哪些地方需要调优呢？
**1. 算法的步长选择** ：步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。   

**2. 算法参数的初始值选择** ：初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 

**3. 归一化** ：由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化。

### 1.5 梯度下降法的缺点

　　**1. 靠近极小值时收敛速度减慢，如下图所示；**

　　**2. 直线搜索时可能会产生一些问题；**

　　**3. 寻找的是局部最优，可能会“之字形”地下降。**
　
### 1.6 梯度下降法大家族（BGD，SGD，MBGD）
### 1.7 批量梯度下降法python实现


-----
引用：
1. 梯度下降法、牛顿法和拟牛顿法：<a target="_blank" href="https://zhuanlan.zhihu.com/p/37524275">https://zhuanlan.zhihu.com/p/37524275 </a>
