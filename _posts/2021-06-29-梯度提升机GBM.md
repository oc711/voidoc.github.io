---
layout: post
title: 机器学习 | 梯度提升机GBM(Grandient Boosting Machine)详解
date: 2021-06-29
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21629.html">原文地址：https://voidoc.blog/21629.html </a>



# 1. 介绍   

**前置知识**：梯度下降法，可参考前序文章<a target="_blank" href="https://voidoc.blog/21627.html">算法基础 | 几种最优化方法详解 </a>  

## 1.1 概述  

Boosting 是集成学习中非常重要的一类算法，其基本原理是串行生成一系列弱学习器（weak learner），这些弱学习器直接通过组合到一起构成最终的模型。Boosting 算法可以用于解决分类和回归问题，主要的算法包括早期的AdaBoost 和 后续的Gradient Boosting。  

> 发展历史  

Boosting类算法的发展历史大致如下：

- 1997年，Yoav Freund 和 Robert Schapire 提出了第一个成功且实用的提升算法 Adaboost 的概念。  
- 1998年，Leo Breiman 将 Adaboost 归纳为建立在特殊损失函数的梯度下降方向上的算法。
- 1999-2001年，Jerome H. Friedman 将梯度下降的思想引入 Boosting 算法，提出了Gradient Boosting 的概念，以便处理不同的损失函数。
- 1999-2000年，Llew Mason，Jonathan Baxter，Peter Bartlett 和 Marcus Frean 也提出了适用于更普遍情况的一般功能性梯度提升 （General Functional Gradient Boosting），通过迭代地选择指向负梯度方向的函数来优化功能空间上的成本函数。
- 2015年，陈天奇 提出了Xgboost的概念并开源使其作为一个研究项目，并用于深度机器学习社区 (DMLC) 。

本文主要关注 Gradient Boosting，以及后来机器学习中常用的GBDT、XGBoost和LightGBM算法等基于GBM的衍生算法。  

AdaBoost 本质上也可以从广义的 Gradient Boosting 推导得到（损失函数使用指数损失），这里暂时不详细展开，后续有时间再拓展。   


# 2. GBM算法

##  2.1. 基本思想  

GBM（Gradient Boosting Machine）算法是Boosting(提升)算法的一种。主要思想是，串行地生成多个弱学习器，每个弱学习器的目标是拟合先前累加模型的损失函数的负梯度， 使加上该弱学习器后的累积模型损失往负梯度的方向减少。 且它用不同的权重将基学习器进行线性组合，使表现优秀的学习器得到重用。 最常用的基学习器为树模型。    

Gradient Boosting 还可以将其理解为**函数空间上的梯度下降**。我们比较熟悉的梯度下降通常是值在参数空间上的梯度下降（如训练神经网络，每轮迭代中计算当前损失关于参数的梯度，对参数进行更新）。  

而在 Gradient Boosting 中，每轮迭代生成一个弱学习器，这个弱学习器拟合损失函数关于之前累积模型的梯度，然后将这个弱学习器加入累积模型中，逐渐降低累积模型的损失。即**参数空间的梯度下降**利用梯度信息调整参数，从而降低损失，而**函数空间的梯度下降**利用梯度，拟合一个新的函数，从而降低损失。  

## 2.2 公式推导  
假设有训练样本{ $x_i,y_i$ }, i=1,...,n,    

在第k-1(k<n)轮获得的累积模型为$F_{m-1}\left ( x \right )$, 则第k轮的弱学习器$h\left ( x \right ) $可以通过下列公式得到：$F_k\left ( x \right )  = F_{k-1}\left ( x \right )+arg\min_{h\subset H}Loss\left ( y_i,  F_{k-1}\left ( x_i \right )+h\left ( x_i \right ) \right ) $    

即，在函数空间H中找到一个弱学习器$h\left ( x \right ) $, 使得，加入了这个弱学习器后，第k轮的累积模型Loss最小。   

已知在k-1轮结束后，我们得到y的预测值$\hat{y} = F_{k-1}\left ( x \right )$, 就可以计算得到损失$Loss\left (y,  F_{k-1}\left ( x \right )   \right ) $, 如果我们希望加入第k轮弱学习器后累积模型的Loss最小，根据梯度下降法，第k轮弱学习器的拟合损失函数应该沿着累计模型$F_{k-1}\left ( x \right ) $的负梯度方向移动，即第k轮弱学习器训练的目标值是累计模型损失函数的负梯度：$g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  



## 2.3 迭代流程  
> **Algorithm : Gradient Boosting**  
1. 初始化$F_{0}\left ( x \right ) = arg\min_{h\subset H}Loss\left (y_i , h\left ( x_i \right )   \right ) $  
2. for k=1:M, Do:
	- 计算累计模型损失函数的负梯度$g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  
	- 拟合弱学习器使得$\sum_{i=1}^{N}\left ( g_k^{i}-h\left ( x_i \right )  \right )^2  $最小化  
	- 更新累积模型$F_k = F_{k-1}+\alpha h\left ( x \right ) $ (其中$\alpha为学习率$)  
3. 直至某个迭代终止条件(如k达到最大迭代次数M)，return $F\left ( x \right ) = F_k\left ( x \right ) $   


以上 Gradient Boosting 的算法流程具有一般性，根据其中的损失函数和弱学习器的不同可以演变出多种不同的算法。  

如果损失函数换成平方损失，则算法变成 L2Boosting；如果将损失函数换成 log-loss，则算法成为 BinomialBoost；如果是指数损失，则算法演变成 AdaBoost，还可以采用 Huber loss 等更加 robust 的损失函数。  

弱学习器如果使用决策树，则算法成为 GBDT（Gradient Boosting Decision Tree）。在下面的章节会展开讲解。  

# 3.GBDT（Gradient Boosting Decision Tree）算法    

GBDT（Gradient Boosting Decision Tree）是弱学习器使用 CART <a target="_blank" href="https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0"> CART </a>  
回归树的一种 Gradient Boosting，使用决策树作为弱学习器的一个好处是：决策树本身是一种不稳定的学习器（训练数据的一点波动可能给结果带来较大的影响），从统计学的角度单棵决策树的方差比较大。而在集成学习中，弱学习器间方差越大，弱学习器本身泛化性能越好，则集成学习模型的泛化性能就越好。因此使用决策树作为弱学习器通常比使用较稳定的弱学习器（如线性回归等）泛化性能更好。  

## 3.1 回归问题    

GBDT 中的每个弱学习器都是 CART 回归树，在回归问题中，损失函数采用均方损失函数：
$Loss\left (y,  F_{m-1}\left ( x \right )   \right ) = \left (y,  F_{m-1}\left ( x \right )   \right )^2$  
那么损失函数的负梯度就为（常数2可以忽略，反正还要乘lr）：  
$g_m = y - F_{m-1}\left ( x \right )$   

代入上面的Algorithm: Gradient Boosting中可以得到 GBDT 回归的流程。核心代码如下：  

```python
def GBDT_fit(self, train_X, train_y): 
    self.estimator_list = list()
    self.F = np.zeros_like(train_y, dtype=float)

    for i in range(1, self.n_estimators + 1):  # 循环逐个生成弱学习器
        # get negative gradients
        neg_grads = train_y - self.F   # 负梯度gm
        base = DecisionTreeRegressor(max_depth=self.max_depth)
        base.fit(train_X, neg_grads)  # 弱学习器向gm拟合，minimize二者间残差
        train_preds = base.predict(train_X)
        self.estimator_list.append(base)  # 将当前弱学习器的预测累加到 F 中
        
        if self.is_first:
            self.F = train_preds
            self.is_first = False
        else:
            self.F += self.lr * train_preds
```  

模型超参数设定为 n_estimators 为 300， max_depth 为 5，lr 为 0.1，数据集选用 BlackFridy，这个数据集总共有 11 维特征，537577 条数据，按照 0.8/0.2 随机划分训练集和验证集。首先我们拿这个模型与 sklearn 的 GradientBoostingRegressor 进行对比如下，可以看到结果差不多。  

> GradientBoostingRegressor train_score: 0.7106 valid_score: 0.7037  
> MyGradientBoostingRegressor train_score: 0.7121 valid_score: 0.7054   

## 3.2 分类问题  

对于分类问题，我们需要对拟合目标稍作转换实现分类。  

基本的思路可以参考线性回归通过对数概率转化为逻辑回归进行分类。逻辑回归也是广义上的线性模型，可以看做是线性回归模型 $\omega x+b$ 去拟合对数概率$\log \frac{p}{1-p} $:  
即：$\omega x+b = \log \frac{p}{1-p}$  

且损失函数是Cross Entropy Loss交叉熵损失，即：$Loss = -y\log p - \left ( 1-y \right ) \log \left (1-p  \right )$

> 推导(信息论角度，还可以从极大似然估计的角度去推导)
> 设事情x发生的概率为p(x), 由信息熵公式可得：$I\left ( x \right ) = -log(p\left ( x \right ) )$  
> 那么在二分类的情况下，模型最后需要预测的结果只有两种情况，设对于每个类别我们的预测得到的概率为p和1-p，此时该事件的熵为两种情况的信息熵累加，即$H\left ( X \right ) = -p\left ( x=0 \right )log\left ( p\left ( x=0 \right ) \right )- p\left ( x=1 \right )log\left ( p\left ( x=1 \right ) \right )$   

在 GBDT 中可以看做是前 k-1 轮的累积模型$F_{k-1}$代替了线性模型$\omega x+b$ 去拟合对数几率，令:  
$F_{k-1} = \log \frac{p_{k-1}}{1-p_{k-1}}$ 两边取exp$\Leftrightarrow p_{k-1} = \frac{1}{1+\exp\left ( -F_{k-1} \right ) }$  
所以代入交叉熵Loss中得：  
$Loss = -y log\left ( \frac{1}{1+\exp \left ( -F_{k-1} \right ) }  \right ) -\left ( 1-y \right )\log \left ( \frac{\exp \left ( -F_{k-1} \right ) }{1+\exp \left ( -F_{k-1} \right )}  \right )\Leftrightarrow 
-y log\left ( 1+\exp\left ( -F_{k-1} \right )  \right ) -\left ( 1-y \right ) \left [ \log \left ( \exp\left ( -F_{k-1} \right )  \right )- \log \left ( 1+\exp \left ( -F_{k-1} \right )  \right )  \right ]$  
展开约项后得，  
$Loss = \left ( 1-y \right ) F_{k-1} + \log \left ( \exp\left ( -F_{k-1} \right )+1  \right )$  
对其求偏导得负梯度：  
$-\frac{\partial Loss}{\partial F_{k-1}} = y - p_{k-1}$  

可以看到最后的负梯度形式十分简洁，将此负梯度作为第k轮的拟合目标，依次不断迭代，GBDT 分类的核心代码如下： 

```python
@staticmethod
def logit(F):
    return 1.0 / (1.0 + np.exp(-F))

def fit(self, train_X, train_y):
    self.estimator_list = list()
    self.F = np.zeros_like(train_y, dtype=float)

    for i in range(1, self.n_estimators + 1):
        # get negative gradients
        neg_grads = train_y - self.logit(self.F)
        base = DecisionTreeRegressor(max_depth=self.max_depth)
        base.fit(train_X, neg_grads)
        train_preds = base.predict(train_X)
        self.estimator_list.append(base)

        if self.is_first:
            self.F = train_preds
            self.is_first = False
        else:
            self.F += self.lr * train_preds
```  

在 heart-disease-uci 数据集中分类的准确率为  

>GradientBoostingRegressor train_score: 0.9834 valid_score: 0.7705  
>MyGradientBoostingRegressor train_score: 0.7171 valid_score: 0.8361

# 4. XGBoost 算法  

**XGBoost（eXtreme Gradient Boosting）极致梯度提升**，是一种基于GBDT的算法或者说工程实现，由陈天奇在论文<a target="_blank" href="https://arxiv.org/pdf/1603.02754.pdf">《 XGBoost：A Scalable Tree Boosting System》</a>  中正式提出。  

## 4.1 XGBoost基本思想  

XGBoost的基本思想和GBDT相同，但是做了一些优化, 主要有： 
- 利用二阶泰勒公式展开使得损失函数更精准  
- 加入正则项避免模型过拟合  
- 采用Block存储结构，可以并行计算  

XGBoost具有高效、灵活和轻便的特点，在数据挖掘、推荐系统等领域得到广泛的应用。  

## 4.2 XGBoost公式推导  
### 4.2.1 XGBoost目标函数推导  

XGBoost的目标函数由损失函数和正则化项两部分组成。  

已知训练数据集$T = { \left (x_1 ,y_1 \right ) ,\dots ,\left ( x_n,y_n \right )} $  
损失函数$l\left ( y_i,\hat{y_i}  \right ) $，正则化项$\Omega \left ( f_k \right ) $。  
则整体目标函数可以写做：$Loss\left ( \Theta \right)=\sum_{i}^{n}l\left ( y_i,\hat{y_i}  \right )+\sum_{k}^{K} \Omega \left ( f_k \right )$, 其中i表示第i个样本，k代表第k棵树。  
而$\hat{y_i}  = \sum_{k=1}^{K} f_k\left ( x_i \right ) $是第i个样本的预测值。 也等于上一轮的预测值+这一轮的，即$\hat{y_i}  = \sum_{k=1}^{K} f_k\left ( x_i \right ) = \hat{y_i} ^{t-1}+f_t\left ( x_i \right ) $   
所以$Loss\left ( \Theta \right)$可以转化成如下形式：  
$Loss^\left ( t \right )=\sum_{i}^{n}l\left ( y_i,\hat{y_i} ^{t-1}+f_t\left ( x_i \right )  \right )+\sum_{k}^{K} \Omega \left ( f_k \right )$  

接下来，我们按照论文中提及的三个步骤优化XGBoost目标函数：  

**1）二阶泰勒展开，去除常数项，优化损失函数项**  

已知f(x)在$x_0$处的二阶泰勒展开为：  
$f\left ( x \right ) \approx f\left ( x_0 \right ) +f'\left ( x_0\right )  \left ( x-x_0 \right )+\frac{{f}''\left ( x_0 \right ) }{2} \left ( x-x_0 \right )^2  $  

假设$l\left ( x \right )=l\left ( y_i,x \right )$, 则对$l\left ( y_i,x \right )$在$x_0$处进行二阶泰勒展开得：  
$l\left ( y_i,x \right ) \approx l\left (y_i, x_0 \right ) +l'\left (y_i,  x_0\right )  \left ( x-x_0 \right )+\frac{{l}''\left ( y_i, x_0 \right ) }{2} \left ( x-x_0 \right )^2$  
类似地，对$l\left ( y_i,x \right )$在$\hat{y_i}^\left (t-1 \right )$处进行二阶泰勒展开得： 
$l\left ( y_i,x \right ) \approx l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right ) +l'\left (y_i,  \hat{y_i}^\left (t-1 \right )\right )  \left ( x-\hat{y_i}^\left (t-1 \right ) \right )+\frac{{l}''\left ( y_i, \hat{y_i}^\left (t-1 \right ) \right ) }{2} \left ( x-\hat{y_i}^\left (t-1 \right )\right )^2   $  
那么令$x=\hat{y_i}^\left (t-1 \right )+f_t\left (x_i\right )$, 且纪一阶导数为$g_i=l'\left (y_i,\hat{y_i}^\left (t-1 \right ) \right )$,二阶导数为$h_i={l}''\left (y_i,\hat{y_i}^\left (t-1 \right ) \right )$  
代入上面地式子就可以得到：  
$l\left ( y_i,\hat{y_i}^\left (t-1 \right )+f_t\left (x_i\right ) \right ) \approx l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right )+g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right )$  
所以代入目标函数可得：  
$Loss^\left ( t \right )=\sum_{i}^{n}\left [l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right )  +g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right ) \right ] +\sum_{k}^{K} \Omega \left ( f_k \right )$  

 





**2) 正则化项展开，去除常数项，优化正则化项**  


**3) 合并一次项系数、二次项系数，得到最终目标函数**  






# 5. LightGBM算法  




# 4.总结   

## GBM算法的优缺点  

- 优点  ：
1.  对于噪声数据不敏感  
2.  能拟合复杂的非线性关系  
3.  精确度较高  
4.  通过控制迭代次数能控制过度拟合  


- 缺点： 
1.  串行计算，不能并行；  
2.  可能会出现过拟合现象； 
3.  设置参数较多；  
4.  抗干扰能力不强。

## GBM与RF比较   


1) RF随机森林通常只需要设置一个超参数即可，每个节点上随机选取的特征数量。一般将该参数设置为特征总数的平方根，模型足以取得不错的效果。而GBM梯度提升机的超参数则包括提升树的数量和深度、学习速率等。   
2) RF随机森林的抗干扰性强，更不容易出现过拟合的情况。而GBM梯度提升机的参数设置不当可能会出现过拟合的情况。   
3) 在某种意义上讲，RF随机森林是一棵比GBM梯度提升机更加灵活的集成树，但在一般情况下，经过良好训练的GBM梯度提升机性能优于随机森林RF。   
4) RF随机森林更容易并行化。但借助于一些高效方法，GBM梯度提升机同样也能实现并行化训练。


GBM 算法可以用于回归模型，同样它也可以用于分类和排名模型。GBM 算法现已被广泛应用于众多领域。 
GBM 算法近年来被提及比较多的一个算法，这主要得益于其算法性能，及该算法在各类数据挖掘以及机器学习比赛中的卓越表现。  






-----


引用：
1. 维基百科——梯度提升技术：<a target="_blank" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF">https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF </a>  
2. Gradient Boosting——borgwang：<a target="_blank" href="https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html">https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html </a>  
3. XGBoost的原理、公式推导、Python实现和应用:<a target="_blank" href="https://zhuanlan.zhihu.com/p/162001079">https://zhuanlan.zhihu.com/p/162001079</a>   

