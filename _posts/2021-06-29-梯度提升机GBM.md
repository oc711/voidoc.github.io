---
layout: post
title: 机器学习 | 梯度提升机GBM(Grandient Boosting Machine)详解
date: 2021-06-29
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21629.html">原文地址：https://voidoc.blog/21629.html </a>



# 1. 介绍   

**前置知识**：梯度下降法，可参考前序文章<a target="_blank" href="https://voidoc.blog/21627.html">算法基础 | 几种最优化方法详解 </a>  

## 1.1 概述  

Boosting 是集成学习中非常重要的一类算法，其基本原理是串行生成一系列弱学习器（weak learner），这些弱学习器直接通过组合到一起构成最终的模型。Boosting 算法可以用于解决分类和回归问题，主要的算法包括早期的AdaBoost 和 后续的Gradient Boosting。  

> 发展历史  

Boosting类算法的发展历史大致如下：

- 1997年，Yoav Freund 和 Robert Schapire 提出了第一个成功且实用的提升算法 Adaboost 的概念。  
- 1998年，Leo Breiman 将 Adaboost 归纳为建立在特殊损失函数的梯度下降方向上的算法。
- 1999-2001年，Jerome H. Friedman 将梯度下降的思想引入 Boosting 算法，提出了Gradient Boosting 的概念，以便处理不同的损失函数。
- 1999-2000年，Llew Mason，Jonathan Baxter，Peter Bartlett 和 Marcus Frean 也提出了适用于更普遍情况的一般功能性梯度提升 （General Functional Gradient Boosting），通过迭代地选择指向负梯度方向的函数来优化功能空间上的成本函数。
- 2015年，陈天奇 提出了Xgboost的概念并开源使其作为一个研究项目，并用于深度机器学习社区 (DMLC) 。

本文主要关注 Gradient Boosting，以及后来机器学习中常用的GBDT、XGBoost和LightGBM算法等基于GBM的衍生算法。  

AdaBoost 本质上也可以从广义的 Gradient Boosting 推导得到（损失函数使用指数损失），这里暂时不详细展开，后续有时间再拓展。   


# 2. GBM算法

##  2.1. 基本思想  

GBM（Gradient Boosting Machine）算法是Boosting(提升)算法的一种。主要思想是，串行地生成多个弱学习器，每个弱学习器的目标是拟合先前累加模型的损失函数的负梯度， 使加上该弱学习器后的累积模型损失往负梯度的方向减少。 且它用不同的权重将基学习器进行线性组合，使表现优秀的学习器得到重用。 最常用的基学习器为树模型。    

Gradient Boosting 还可以将其理解为**函数空间上的梯度下降**。我们比较熟悉的梯度下降通常是值在参数空间上的梯度下降（如训练神经网络，每轮迭代中计算当前损失关于参数的梯度，对参数进行更新）。  

而在 Gradient Boosting 中，每轮迭代生成一个弱学习器，这个弱学习器拟合损失函数关于之前累积模型的梯度，然后将这个弱学习器加入累积模型中，逐渐降低累积模型的损失。即**参数空间的梯度下降**利用梯度信息调整参数，从而降低损失，而**函数空间的梯度下降**利用梯度，拟合一个新的函数，从而降低损失。  

## 2.3 公式推导  
假设有训练样本{ $x_i,y_i$ }$, i=1,...,n  

在第 k-1 (k<n) 轮获得的累积模型为 $F_{m-1}\left ( x \right )$, 则第k轮的弱学习器$h\left ( x \right ) $可以通过下列公式得到：  
    $F_k\left ( x \right )  = F_{k-1}\left ( x \right )+arg\min_{h\subset H}Loss\left ( y_i,  F_{k-1}\left ( x_i \right )+h\left ( x_i \right ) \right ) $   
即，在函数空间H中找到一个弱学习器$h\left ( x \right ) $, 使得，加入了这个弱学习器后，第k轮的累积模型Loss最小。  
已知在k-1轮结束后，我们得到y的预测值$\hat{y} = F_{k-1}\left ( x \right )  $, 就可以计算得到损失$Loss\left (y,  F_{k-1}\left ( x \right )   \right ) $, 如果我们希望加入第k轮弱学习器后累积模型的Loss最小，根据梯度下降法，第k轮弱学习器的拟合损失函数应该沿着累计模型$F_{k-1}\left ( x \right ) $的负梯度方向移动，即第k轮弱学习器训练的目标值是累计模型损失函数的负梯度： 
    $g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  


## 2.4 迭代流程  
> **Algorithm : Gradient Boosting**  
1. 初始化$F_{0}\left ( x \right ) = arg\min_{h\subset H}Loss\left (y_i , h\left ( x_i \right )   \right ) $  
2. for k=1:M, Do:
	- 计算累计模型损失函数的负梯度$g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  
	- 拟合弱学习器使得$\sum_{i=1}^{N}\left ( g_k^{i}-h\left ( x_i \right )  \right )^2  $最小化  
	- 更新累积模型$F_k = F_{k-1}+\alpha h\left ( x \right ) $ (其中$\alpha为学习率$)  
3. 直至某个迭代终止条件(如k达到最大迭代次数M)，return $F\left ( x \right ) = F_k\left ( x \right ) $   



以上 Gradient Boosting 的算法流程具有一般性，根据其中的损失函数和弱学习器的不同可以演变出多种不同的算法。  

如果损失函数换成平方损失，则算法变成 L2Boosting；如果将损失函数换成 log-loss，则算法成为 BinomialBoost；如果是指数损失，则算法演变成 AdaBoost；  

还可以采用 Huber loss 等更加 robust 的损失函数。弱学习器如果使用决策树，则算法成为 GBDT（Gradient Boosting Decision Tree）。在下面的章节会展开讲解。  

# 3.GBDT（Gradient Boosting Decision Tree）算法




# 4.总结   

## GBM算法的优缺点：  

- 优点  ：
1.  对于噪声数据不敏感  
2.  能拟合复杂的非线性关系  
3.  精确度较高  
4.  通过控制迭代次数能控制过度拟合  


- 缺点： 
1.  串行计算，不能并行；  
2.  可能会出现过拟合现象； 
3.  设置参数较多；  
4.  抗干扰能力不强。

## GBM与RF比较： 


1) RF随机森林通常只需要设置一个超参数即可，每个节点上随机选取的特征数量。一般将该参数设置为特征总数的平方根，模型足以取得不错的效果。而GBM梯度提升机的超参数则包括提升树的数量和深度、学习速率等。   
2) RF随机森林的抗干扰性强，更不容易出现过拟合的情况。而GBM梯度提升机的参数设置不当可能会出现过拟合的情况。   
3) 在某种意义上讲，RF随机森林是一棵比GBM梯度提升机更加灵活的集成树，但在一般情况下，经过良好训练的GBM梯度提升机性能优于随机森林RF。   
4) RF随机森林更容易并行化。但借助于一些高效方法，GBM梯度提升机同样也能实现并行化训练。


GBM 算法可以用于回归模型，同样它也可以用于分类和排名模型。GBM 算法现已被广泛应用于众多领域。 
GBM 算法近年来被提及比较多的一个算法，这主要得益于其算法性能，及该算法在各类数据挖掘以及机器学习比赛中的卓越表现。  






-----


引用：
1. 维基百科——梯度提升技术：<a target="_blank" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF">https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF </a>  
2. Gradient Boosting——borgwang：<a target="_blank" href="https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html">https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html </a>  
