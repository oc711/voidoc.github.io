---
layout: post
title: 机器学习 | 梯度提升机GBM(Grandient Boosting Machine)详解
date: 2021-06-29
tag: 工作
---   

<img src="/images/copyright.ico" alt="copyright" style="display:inline;margin-bottom: -5px;" width="20" height="20"> 版权声明：本文为博主原创文章，未经博主允许不得转载。
<a target="_blank" href="https://voidoc.blog/21629.html">原文地址：https://voidoc.blog/21629.html </a>



# 1. 介绍   

**前置知识**：梯度下降法，可参考前序文章<a target="_blank" href="https://voidoc.blog/21627.html">算法基础 | 几种最优化方法详解 </a>  

## 1.1 概述  

Boosting 是集成学习中非常重要的一类算法，其基本原理是串行生成一系列弱学习器（weak learner），这些弱学习器直接通过组合到一起构成最终的模型。Boosting 算法可以用于解决分类和回归问题，主要的算法包括早期的AdaBoost 和 后续的Gradient Boosting。  

> 发展历史  

Boosting类算法的发展历史大致如下：

- 1997年，Yoav Freund 和 Robert Schapire 提出了第一个成功且实用的提升算法 Adaboost 的概念。  
- 1998年，Leo Breiman 将 Adaboost 归纳为建立在特殊损失函数的梯度下降方向上的算法。
- 1999-2001年，Jerome H. Friedman 将梯度下降的思想引入 Boosting 算法，提出了Gradient Boosting 的概念，以便处理不同的损失函数。
- 1999-2000年，Llew Mason，Jonathan Baxter，Peter Bartlett 和 Marcus Frean 也提出了适用于更普遍情况的一般功能性梯度提升 （General Functional Gradient Boosting），通过迭代地选择指向负梯度方向的函数来优化功能空间上的成本函数。
- 2015年，陈天奇 提出了Xgboost的概念并开源使其作为一个研究项目，并用于深度机器学习社区 (DMLC) 。

本文主要关注 Gradient Boosting，以及后来机器学习中常用的GBDT、XGBoost和LightGBM算法等基于GBM的衍生算法。  

AdaBoost 本质上也可以从广义的 Gradient Boosting 推导得到（损失函数使用指数损失），这里暂时不详细展开，后续有时间再拓展。   


# 2. GBM算法

##  2.1. 基本思想  

GBM（Gradient Boosting Machine）算法是Boosting(提升)算法的一种。主要思想是，串行地生成多个弱学习器，每个弱学习器的目标是拟合先前累加模型的损失函数的负梯度， 使加上该弱学习器后的累积模型损失往负梯度的方向减少。 且它用不同的权重将基学习器进行线性组合，使表现优秀的学习器得到重用。 最常用的基学习器为树模型。    

Gradient Boosting 还可以将其理解为**函数空间上的梯度下降**。我们比较熟悉的梯度下降通常是值在参数空间上的梯度下降（如训练神经网络，每轮迭代中计算当前损失关于参数的梯度，对参数进行更新）。  

而在 Gradient Boosting 中，每轮迭代生成一个弱学习器，这个弱学习器拟合损失函数关于之前累积模型的梯度，然后将这个弱学习器加入累积模型中，逐渐降低累积模型的损失。即**参数空间的梯度下降**利用梯度信息调整参数，从而降低损失，而**函数空间的梯度下降**利用梯度，拟合一个新的函数，从而降低损失。  

## 2.2 公式推导  
假设有训练样本{ $x_i,y_i$ }, i=1,...,n,    

在第k-1(k<n)轮获得的累积模型为$F_{m-1}\left ( x \right )$, 则第k轮的弱学习器$h\left ( x \right ) $可以通过下列公式得到：$F_k\left ( x \right )  = F_{k-1}\left ( x \right )+arg\min_{h\subset H}Loss\left ( y_i,  F_{k-1}\left ( x_i \right )+h\left ( x_i \right ) \right ) $    

即，在函数空间H中找到一个弱学习器$h\left ( x \right ) $, 使得，加入了这个弱学习器后，第k轮的累积模型Loss最小。   

已知在k-1轮结束后，我们得到y的预测值$\hat{y} = F_{k-1}\left ( x \right )$, 就可以计算得到损失$Loss\left (y,  F_{k-1}\left ( x \right )   \right ) $, 如果我们希望加入第k轮弱学习器后累积模型的Loss最小，根据梯度下降法，第k轮弱学习器的拟合损失函数应该沿着累计模型$F_{k-1}\left ( x \right ) $的负梯度方向移动，即第k轮弱学习器训练的目标值是累计模型损失函数的负梯度：$g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  



## 2.3 迭代流程  
> **Algorithm : Gradient Boosting**  
1. 初始化$F_{0}\left ( x \right ) = arg\min_{h\subset H}Loss\left (y_i , h\left ( x_i \right )   \right ) $  
2. for k=1:M, Do:
	- 计算累计模型损失函数的负梯度$g_k = - \frac{\partial Loss\left ( y,F_{k-1}\left ( x \right )  \right ) }{\partial F_{k-1}\left ( x \right )} $  
	- 拟合弱学习器使得$\sum_{i=1}^{N}\left ( g_k^{i}-h\left ( x_i \right )  \right )^2  $最小化  
	- 更新累积模型$F_k = F_{k-1}+\alpha h\left ( x \right ) $ (其中$\alpha为学习率$)  
3. 直至某个迭代终止条件(如k达到最大迭代次数M)，return $F\left ( x \right ) = F_k\left ( x \right ) $   


以上 Gradient Boosting 的算法流程具有一般性，根据其中的损失函数和弱学习器的不同可以演变出多种不同的算法。  

如果损失函数换成平方损失，则算法变成 L2Boosting；如果将损失函数换成 log-loss，则算法成为 BinomialBoost；如果是指数损失，则算法演变成 AdaBoost，还可以采用 Huber loss 等更加 robust 的损失函数。  

弱学习器如果使用决策树，则算法成为 GBDT（Gradient Boosting Decision Tree）。在下面的章节会展开讲解。  

# 3.GBDT（Gradient Boosting Decision Tree）算法    

GBDT（Gradient Boosting Decision Tree）是弱学习器使用 CART <a target="_blank" href="https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0"> CART </a>  
回归树的一种 Gradient Boosting，使用决策树作为弱学习器的一个好处是：决策树本身是一种不稳定的学习器（训练数据的一点波动可能给结果带来较大的影响），从统计学的角度单棵决策树的方差比较大。而在集成学习中，弱学习器间方差越大，弱学习器本身泛化性能越好，则集成学习模型的泛化性能就越好。因此使用决策树作为弱学习器通常比使用较稳定的弱学习器（如线性回归等）泛化性能更好。  

## 3.1 回归问题    

GBDT 中的每个弱学习器都是 CART 回归树，在回归问题中，损失函数采用均方损失函数：
$Loss\left (y,  F_{m-1}\left ( x \right )   \right ) = \left (y,  F_{m-1}\left ( x \right )   \right )^2$  
那么损失函数的负梯度就为（常数2可以忽略，反正还要乘lr）：  
$g_m = y - F_{m-1}\left ( x \right )$   

代入上面的Algorithm: Gradient Boosting中可以得到 GBDT 回归的流程。核心代码如下：  

```python
def GBDT_fit(self, train_X, train_y): 
    self.estimator_list = list()
    self.F = np.zeros_like(train_y, dtype=float)

    for i in range(1, self.n_estimators + 1):  # 循环逐个生成弱学习器
        # get negative gradients
        neg_grads = train_y - self.F   # 负梯度gm
        base = DecisionTreeRegressor(max_depth=self.max_depth)
        base.fit(train_X, neg_grads)  # 弱学习器向gm拟合，minimize二者间残差
        train_preds = base.predict(train_X)
        self.estimator_list.append(base)  # 将当前弱学习器的预测累加到 F 中
        
        if self.is_first:
            self.F = train_preds
            self.is_first = False
        else:
            self.F += self.lr * train_preds
```

模型超参数设定为 n_estimators 为 300， max_depth 为 5，lr 为 0.1，数据集选用 BlackFridy，这个数据集总共有 11 维特征，537577 条数据，按照 0.8/0.2 随机划分训练集和验证集。首先我们拿这个模型与 sklearn 的 GradientBoostingRegressor 进行对比如下，可以看到结果差不多。  

> GradientBoostingRegressor train_score: 0.7106 valid_score: 0.7037  
> MyGradientBoostingRegressor train_score: 0.7121 valid_score: 0.7054   

## 3.2 分类问题  

对于分类问题，我们需要对拟合目标稍作转换实现分类。  

基本的思路可以参考线性回归通过对数概率转化为逻辑回归进行分类。逻辑回归也是广义上的线性模型，可以看做是线性回归模型 $\omega x+b$ 去拟合对数概率$\log \frac{p}{1-p} $:  
即：$\omega x+b = \log \frac{p}{1-p}$  

且损失函数是Cross Entropy Loss交叉熵损失，即：$Loss = -y\log p - \left ( 1-y \right ) \log \left (1-p  \right )$

> 推导(信息论角度，还可以从极大似然估计的角度去推导)
> 设事情x发生的概率为p(x), 由信息熵公式可得：$I\left ( x \right ) = -log(p\left ( x \right ) )$  
> 那么在二分类的情况下，模型最后需要预测的结果只有两种情况，设对于每个类别我们的预测得到的概率为p和1-p，此时该事件的熵为两种情况的信息熵累加，即$H\left ( X \right ) = -p\left ( x=0 \right )log\left ( p\left ( x=0 \right ) \right )- p\left ( x=1 \right )log\left ( p\left ( x=1 \right ) \right )$   

在 GBDT 中可以看做是前 k-1 轮的累积模型$F_{k-1}$代替了线性模型$\omega x+b$ 去拟合对数几率，令:  
$F_{k-1} = \log \frac{p_{k-1}}{1-p_{k-1}}$ 两边取exp$\Leftrightarrow p_{k-1} = \frac{1}{1+\exp\left ( -F_{k-1} \right ) }$  
所以代入交叉熵Loss中得：  
$Loss = -y log\left ( \frac{1}{1+\exp \left ( -F_{k-1} \right ) }  \right ) -\left ( 1-y \right )\log \left ( \frac{\exp \left ( -F_{k-1} \right ) }{1+\exp \left ( -F_{k-1} \right )}  \right )\Leftrightarrow 
-y log\left ( 1+\exp\left ( -F_{k-1} \right )  \right ) -\left ( 1-y \right ) \left [ \log \left ( \exp\left ( -F_{k-1} \right )  \right )- \log \left ( 1+\exp \left ( -F_{k-1} \right )  \right )  \right ]$  
展开约项后得，  
$Loss = \left ( 1-y \right ) F_{k-1} + \log \left ( \exp\left ( -F_{k-1} \right )+1  \right )$  
对其求偏导得负梯度：  
$-\frac{\partial Loss}{\partial F_{k-1}} = y - p_{k-1}$  

可以看到最后的负梯度形式十分简洁，将此负梯度作为第k轮的拟合目标，依次不断迭代，GBDT 分类的核心代码如下： 

```python
@staticmethod
def logit(F):
    return 1.0 / (1.0 + np.exp(-F))

def fit(self, train_X, train_y):
    self.estimator_list = list()
    self.F = np.zeros_like(train_y, dtype=float)

    for i in range(1, self.n_estimators + 1):
        # get negative gradients
        neg_grads = train_y - self.logit(self.F)
        base = DecisionTreeRegressor(max_depth=self.max_depth)
        base.fit(train_X, neg_grads)
        train_preds = base.predict(train_X)
        self.estimator_list.append(base)

        if self.is_first:
            self.F = train_preds
            self.is_first = False
        else:
            self.F += self.lr * train_preds
```

在 heart-disease-uci 数据集中分类的准确率为  

>GradientBoostingRegressor train_score: 0.9834 valid_score: 0.7705  
>MyGradientBoostingRegressor train_score: 0.7171 valid_score: 0.8361

# 4. XGBoost 算法  

**XGBoost（eXtreme Gradient Boosting）极致梯度提升**，是一种基于GBDT的算法或者说工程实现，由陈天奇在论文<a target="_blank" href="https://arxiv.org/pdf/1603.02754.pdf">《 XGBoost：A Scalable Tree Boosting System》</a>  中正式提出。  

## 4.1 XGBoost基本思想  

XGBoost的基本思想和GBDT相同，但是做了一些优化, 主要有： 
- 利用二阶泰勒公式展开使得损失函数更精准  
- 加入正则项避免模型过拟合  
- 采用Block存储结构，可以并行计算  

XGBoost具有高效、灵活和轻便的特点，在数据挖掘、推荐系统等领域得到广泛的应用。  

## 4.2 XGBoost公式推导  
### 4.2.1 XGBoost目标函数推导  

XGBoost的目标函数由损失函数和正则化项两部分组成。  

已知训练数据集$T = { \left (x_1 ,y_1 \right ) ,\dots ,\left ( x_n,y_n \right )} $  
损失函数$l\left ( y_i,\hat{y_i}  \right ) $，正则化项$\Omega \left ( f_k \right ) $。  
则整体目标函数可以写做：$Loss\left ( \Theta \right)=\sum_{i}^{n}l\left ( y_i,\hat{y_i}  \right )+\sum_{k}^{K} \Omega \left ( f_k \right )$, 其中i表示第i个样本，k代表第k棵树。  
而$\hat{y_i}  = \sum_{k=1}^{K} f_k\left ( x_i \right ) $是第i个样本的预测值。 也等于上一轮的预测值+这一轮的，即$\hat{y_i}  = \sum_{k=1}^{K} f_k\left ( x_i \right ) = \hat{y_i} ^{t-1}+f_t\left ( x_i \right ) $   
所以$Loss\left ( \Theta \right)$可以转化成如下形式：  
$Loss^\left ( t \right )=\sum_{i}^{n}l\left ( y_i,\hat{y_i} ^{t-1}+f_t\left ( x_i \right )  \right )+\sum_{k}^{K} \Omega \left ( f_k \right )$  

接下来，我们按照论文中提及的三个步骤优化XGBoost目标函数：  

**1）二阶泰勒展开，去除常数项，优化损失函数项**  

已知f(x)在$x_0$处的二阶泰勒展开为：  
$f\left ( x \right ) \approx f\left ( x_0 \right ) +f'\left ( x_0\right )  \left ( x-x_0 \right )+\frac{f'' \left ( x_0 \right ) }{2} \left ( x-x_0 \right )^2  $  

假设$l\left ( x \right )=l\left ( y_i,x \right )$, 则对$l\left ( y_i,x \right )$在$x_0$处进行二阶泰勒展开得：  
$l\left ( y_i,x \right ) \approx l\left (y_i, x_0 \right ) +l'\left (y_i,  x_0\right )  \left ( x-x_0 \right )+\frac{l''\left ( y_i, x_0 \right ) }{2} \left ( x-x_0 \right )^2$  
类似地，对$l\left ( y_i,x \right )$在$\hat{y_i}^\left (t-1 \right )$处进行二阶泰勒展开得： 
$$l\left ( y_i,x \right ) \approx l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right ) +l'\left (y_i,  \hat{y_i}^\left (t-1 \right )\right )  \left ( x-\hat{y_i}^\left (t-1 \right ) \right )+\frac{l''\left ( y_i, \hat{y_i}^\left (t-1 \right ) \right ) }{2} \left ( x-\hat{y_i}^\left (t-1 \right )\right )^2$$  
那么令$x=\hat{y_i}^\left (t-1 \right )+f_t\left (x_i\right )$, 且记一阶导数为$$g_i=l'\left (y_i,\hat{y_i}^\left (t-1 \right ) \right )$,二阶导数为$h_i=l''\left (y_i,\hat{y_i}^\left (t-1 \right ) \right )$$  
代入上面地式子就可以得到：  
$$l\left ( y_i,\hat{y_i}^\left (t-1 \right )+f_t\left (x_i\right ) \right ) \approx l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right )+g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right )$$  
所以代入目标函数可得：  
$$Loss^\left ( t \right )=\sum_{i}^{n}\left [l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right )  +g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right ) \right ] +\sum_{k}^{K} \Omega \left ( f_k \right )$$   


**2) 正则化项展开，去除常数项，优化正则化项**    
其中$l\left (y_i, \hat{y_i}^\left (t-1 \right ) \right ) $是一个常数项，可以移除以简化目标函数至：  
$$Loss^\left ( t \right )=\sum_{i}^{n}\left [g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right ) \right ] +\sum_{k}^{K} \Omega \left ( f_k \right )$$  
将正则项进行拆分：  
$$\sum_{k}^{K} \Omega \left ( f_k \right )=\sum_{k=1}^{t} \Omega \left ( f_k \right )=\Omega \left ( f_t \right)+\sum_{k=1}^{t-1}\Omega \left ( f_k \right ) $$  
且因为t-1棵树的结构已经确定，所以可以看作一个常数，可以移除，即：  
$$Loss^\left ( t \right )=\sum_{i}^{n}\left [ g_if_t\left ( x_i \right ) +\frac{h_i}{2} f_t^2\left ( x_i \right ) \right ] + \Omega \left ( f_t \right )  $$     

**3) 合并一次项系数、二次项系数，得到最终目标函数**   

- 首先需要重新定义一棵树，包括两个部分：
1. 叶子节点的权重向量$\omega \subset R^T$     
2. 叶子节点的映射关系$q:R^d\to {1,2,\dots,T}$, T为这棵树的叶子节点的数量。   
则一棵树的表达形式为：$f_t\left ( x \right ) =\omega_{q\left ( x \right ) }$   
<img src="https://www.imageoss.com/images/2021/07/01/self1db24ea453105d9a6.jpg" alt="self1" border="0">  

-  然后定义一棵树的复杂度$\Omega$, 也包括两个部分：  
1. 叶子节点的数量T  
2. 叶子节点权重向量的L2范数  
所以一棵树的复杂度的表示为： $\Omega \left ( f_t \right ) = \gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T}\omega _j^2 $   
<img src="https://www.imageoss.com/images/2021/07/01/self25560e842479d861d.jpg" alt="self2" border="0">  


假设属于第j个叶子节点的所有样本$x_i$,划入到一个叶子节点样本集合中，即  $I_j = {i|q\left ( x_i \right ) =j}$  ，将$f_t\left ( x_i \right ) = \omega_{q\left ( x_i \right ) }$代入目标函数，得：  
$$Loss^\left ( t \right )=\sum_{j=1}^{T}\left [\left (\sum_{i\in I_j}^{} g_i \right )w_j +\frac{1}{2} \left ( \sum_{i\in I_j}^{} h_i+\lambda  \right )w_j^2 \right ] + \gamma T$$  

最后，合并一次项系数、二次项系数，得：   
$$Loss^\left ( t \right )=\sum_{j=1}^{T}\left [G_jw_j +\frac{1}{2} \left ( H_j+\lambda  \right )w_j^2 \right ] + \gamma T$$ 即得 **XGBoost的最终目标函数**   
其中$G_j$是所有叶子节点j所包含样本得一阶偏导数累加和，是一个常数；   
$H_j$是所有叶子节点j所包含样本得二阶偏导数累加和，也是一个常数。   


### 4.2.2 目标函数的最优解    
已知XGBoost的目标函数：$$Loss^\left ( t \right )=\sum_{j=1}^{T}\left [G_jw_j +\frac{1}{2} \left ( H_j+\lambda  \right )w_j^2 \right ] + \gamma T$$   
则每个叶子节点j的目标函数为: $f\left ( w_j \right ) = G_jw_j +\frac{1}{2} \left ( H_j+\lambda  \right )w_j^2$，是一个关于$w_j$的一元二次函数。  
那么根据求根公式，我们可知这个函数在$w_j=-\frac{G_j}{H_j+\lambda } $处取得最小值$f\left (  w_j \right )=-\frac{G_j^2}{2\left (H_j+\lambda  \right )  } $  此时目标函数的目标值Obj最小，树结构最好，即是目标函数的最优解：  
$$Obj=-\frac{1}{2}\sum_{j=1}^{T}  \frac{G_j^2}{H_j+\lambda  }+\gamma T$$

# 5. LightGBM算法  

## 5.1 LightGBM基本思想    

LightGBM是微软旗下的Distributed Machine Learning Toolkit （DMKT）的一个项目，由2014年首届阿里巴巴大数据竞赛获胜者之一柯国霖主持开发。虽然其开源时间才仅仅2个月，但是其快速高效的特点已经在数据科学竞赛中崭露头角。Allstate Claims Severity竞赛中的冠军解决方案里就使用了LightGBM，并对其大嘉赞赏。  


LightGBM的设计思路主要是两点：
1. 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据；  
2. 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。  

由此可见，LightGBM的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。   

## 5.2 LightGBM和XGBoost的差异 

二者都采用损失函数的负梯度作为当前决策树的残差近似值，去拟合新的决策树。  

LightGBM树的生长方式是垂直方向的，其他的算法都是水平方向的，也就是说Light GBM生长的是树的叶子，其他的算法生长的是树的层次。  

LightGBM选择具有最大误差的树叶进行生长，当生长同样的树叶，生长叶子的算法可以比基于层的算法减少更多的loss。  

**不建议在小数据集上使用LightGBM**, 因为LightGBM对过拟合很敏感，对于小数据集非常容易过拟合。建议对于10000+以上的数据的时候，再使用LightGBM。  

LightGBM在很多方面会比XGBoost表现的更为优秀。它有以下优势：
- 更快的训练效率    
- 低内存使用    
- 更高的准确率    
- 支持并行化学习    
- 可处理大规模数据    
- 支持直接使用category特征      

实验数据表明， LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。  



# 6.Boosting与Bagging的区别  

**1）样本选择上**    
	- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。   
	- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。   

**2）样例权重**       
	- Bagging：使用均匀取样，每个样例的权重相等   
	- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

**3）预测函数**        
	- Bagging：所有预测函数的权重相等。
	- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

**4）并行计算**       
	- Bagging：各个预测函数可以并行生成     
	- Boosting：理论上各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。计算角度来看，两种方法都可以并行。bagging从框架原理上天然适合并行计算。boosting有强力工具stochastic gradient boosting。  

**5）bagging是减少variance，而boosting是减少bias**       

在机器学习中，我们用训练数据集去训练（学习）一个model（模型），通常的做法是定义一个Loss function（误差函数），通过将这个Loss（或者叫error）的最小化过程，来提高模型的性能（performance）。然而我们学习一个模型的目的是为了解决实际的问题（或者说是训练数据集这个领域（field）中的一般化问题），单纯地将训练数据集的loss最小化，并不能保证在解决更一般的问题时模型仍然是最优，甚至不能保证模型是可用的。这个训练数据集的loss与一般化的数据集的loss之间的差异就叫做generalization error。而generalization error又可以细分为Bias和Variance两个部分。  即error=Bias+Variance。   

那么我们就可以通过降低Bias 或者 降低Variance来减小error

Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）   

所以bagging后的bias和单个子模型的接近，因为期望有$E\left ( \frac{\sum X_i }{n}  \right ) = E\left ( X_i \right )$的特性, 所以一般来说不能显著降低bias。   

另一方面，若各子模型独立，则有$Var\left ( \frac{\sum X_i }{n}  \right ) = \frac{Var\left ( X_i \right ) }{n} $可以显著降低variance。  

而boosting是在sequential地最小化损失函数，其bias自然逐步下降。但由于是采取这种sequential、adaptive的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低variance。所以说boosting主要还是靠降低bias来提升预测精度。  





# 7.总结     


GBM 算法可以用于回归模型，同样它也可以用于分类和排名模型。 是近年来被提及比较多的一个算法，这主要得益于其算法性能，及该算法的优化衍生算法，以及其在各类数据挖掘以及机器学习比赛中的卓越表现。   






**引用**  

1. 维基百科——梯度提升技术：<a target="_blank" href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF">https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%8A%80%E6%9C%AF </a>  
2. Gradient Boosting——borgwang：<a target="_blank" href="https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html">https://borgwang.github.io/ml/2019/04/12/gradient-boosting.html </a>  
3. XGBoost的原理、公式推导、Python实现和应用:<a target="_blank" href="https://zhuanlan.zhihu.com/p/162001079">https://zhuanlan.zhihu.com/p/162001079</a>   

